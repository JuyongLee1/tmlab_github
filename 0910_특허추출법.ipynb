{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14DKu9BfZTEk-NZn-LxKctFXb5eTz7aNH",
      "authorship_tag": "ABX9TyMz1m6orOEVOoVfb4hkFFL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuyongLee1/tmlab_github/blob/main/0910_%ED%8A%B9%ED%97%88%EC%B6%94%EC%B6%9C%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지 가져오기\n",
        "import pandas as pd\n",
        "import requests\n",
        "import progressbar\n",
        "import time\n",
        "import os\n",
        "from os.path import join\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle"
      ],
      "metadata": {
        "id": "wDxs7BK6kOTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "script_path = \"/content/drive/My Drive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM-KESssnBmU",
        "outputId": "132a2851-a9c3-4b59-f975-a113da65c226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search-gp.csv 파일을 데이터프레임으로 불러오기\n",
        "search_df = pd.read_csv(join(script_path, 'gp-search_0910.csv'), skiprows=[0])\n"
      ],
      "metadata": {
        "id": "ZYsbTLOnouvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 부분은 코드에 재개 기능을 추가합니다. 이전 실행의 결과를 코드 경로에서 불러와 결과의 마지막 인덱스에서부터 search-gp.csv를 슬라이싱합니다.\n",
        "if os.path.isfile(join(script_path, 'patents_data_0910.csv')):\n",
        "    result = pd.read_csv(join(script_path, 'patents_data_0910.csv'), index_col=0)\n",
        "    search_df = search_df.loc[result.index[-1] + 1:, :]\n",
        "else:\n",
        "    result = pd.DataFrame(columns=['ID', 'Title', 'Abstract', 'Description', 'Claims', 'Inventors', 'Current Assignee', 'Patent Office', 'Publication Date', 'URL'])\n",
        "\n",
        "# 스크랩되지 않은 링크 목록을 불러옵니다.\n",
        "if os.path.isfile(join(script_path, 'not_scrap_pickle_0910')):\n",
        "    with open(join(script_path, 'not_scrap_pickle_0910'), 'rb') as fp:\n",
        "        not_scraped = pickle.load(fp)\n",
        "else:\n",
        "    not_scraped = []"
      ],
      "metadata": {
        "id": "zYM43FVvoydO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 요청에 대해 Google 서버로 전송할 사용자 에이전트 설정\n",
        "h = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}\n",
        "# h = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}"
      ],
      "metadata": {
        "id": "XtFM6fNTo0fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search-gp.csv의 각 행을 반복하면서 서버에 요청을 보냅니다.\n",
        "for (index, row), i in zip(search_df.iterrows(), progressbar.progressbar(range(len(search_df)))):\n",
        "    link = row['result link']\n",
        "    # Google Patents로 요청을 보내고 특허 페이지의 소스를 스크랩합니다.\n",
        "    try:\n",
        "        r = requests.get(link, headers=h)\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        not_scraped.append(link)\n",
        "        print(e, '\\n\\n')\n",
        "        # 에러 비율이 20% 이상인 경우 프로그램을 종료합니다.\n",
        "        if len(not_scraped) / int(index) >= 0.2:\n",
        "            print('\\n요청 중 절반 이상이 에러로 인해 실패했습니다. 이유를 확인하려면 출력 내용을 읽어보세요.\\n')\n",
        "            break\n",
        "        continue\n",
        "    # BeautifulSoup을 사용하여 HTML에서 정보 추출\n",
        "    bs = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    abst = bs.find('section', {'itemprop': 'abstract'})\n",
        "    # 요약이 없는 경우 처리\n",
        "    if abst is not None:\n",
        "        # 요약에 비영어 문단이 있는 경우 처리\n",
        "        if abst.find('span', class_='notranslate') is None:\n",
        "            abst = abst.text.strip()\n",
        "        else:\n",
        "            notranslate = [tag.find(class_='google-src-text') for tag in abst.find_all('span', class_='notranslate')]\n",
        "            for tag in notranslate:\n",
        "                tag.extract()\n",
        "            abst = abst.text.strip()\n",
        "    else:\n",
        "        abst = 'Not Found'\n",
        "\n",
        "    # 클레임 섹션 찾기\n",
        "    claims = bs.find('section', {'itemprop': 'claims'})\n",
        "    # 클레임이 없는 경우 처리\n",
        "    if claims is not None:\n",
        "        # 클레임에 비영어 문단이 있는 경우 처리\n",
        "        if claims.find('span', class_='notranslate') is None:\n",
        "            claims = claims.text.strip()\n",
        "        else:\n",
        "            notranslate = [tag.find(class_='google-src-text') for tag in claims.find_all('span', class_='notranslate')]\n",
        "            for tag in notranslate:\n",
        "                tag.extract()\n",
        "            claims = claims.text.strip()\n",
        "    else:\n",
        "        claims = 'Not Found'\n",
        "\n",
        "     # 결과 데이터프레임에 정보 추가\n",
        "    result.at[index, 'ID'] = search_df.at[index, 'id']\n",
        "    result.at[index, 'Title'] = search_df.at[index, 'title']\n",
        "    result.at[index, 'Abstract'] = abst\n",
        "    result.at[index, 'Claims'] = claims\n",
        "    result.at[index, 'Inventors'] = search_df.at[index, 'inventor/author']\n",
        "    result.at[index, 'Current Assignee'] = search_df.at[index, 'assignee']\n",
        "    result.at[index, 'Publication Date'] = search_df.at[index, 'publication date']\n",
        "    result.at[index, 'URL'] = search_df.at[index, 'result link']\n",
        "\n",
        "    # 결과 데이터프레임과 스크랩되지 않은 링크 목록을 5번 반복할 때마다 저장합니다.\n",
        "    #if i % 5 == 0:\n",
        "    # result.to_csv(join(script_path, 'patents_data.csv'))\n",
        "   #  with open(join(script_path, 'not_scrap_pickle'), 'wb') as fp:\n",
        "   #     pickle.dump(not_scraped, fp)\n",
        "    # Google 차단을 피하기 위해 10번 반복할 때마다 5초 동안 대기합니다.\n",
        "    #if i % 10 == 0 and i != 0:\n",
        "       # time.sleep(5)\n",
        "\n",
        "# 결과 데이터프레임과 스크랩되지 않은 링크 목록을 최종 저장합니다.\n",
        "result.to_csv(join(script_path, 'patents_data_0910.csv'))\n",
        "with open(join(script_path, 'not_scrap_pickle_0910'), 'wb') as fp:\n",
        "    pickle.dump(not_scraped, fp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYKm4A_o2W8",
        "outputId": "c5a36fe3-06f0-49f3-b246-ca6f1a849a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99% (5175 of 5176) |################### | Elapsed Time: 0:35:11 ETA:   0:00:00"
          ]
        }
      ]
    }
  ]
}